{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 s, sys: 351 ms, total: 1.66 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "      <th>pc5</th>\n",
       "      <th>pc6</th>\n",
       "      <th>pc7</th>\n",
       "      <th>pc8</th>\n",
       "      <th>pc9</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc11</th>\n",
       "      <th>pc12</th>\n",
       "      <th>pc13</th>\n",
       "      <th>pc14</th>\n",
       "      <th>pc15</th>\n",
       "      <th>diagadhd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.316805</td>\n",
       "      <td>-3.993883</td>\n",
       "      <td>2.608049</td>\n",
       "      <td>-1.799177</td>\n",
       "      <td>0.421655</td>\n",
       "      <td>-0.185647</td>\n",
       "      <td>-0.866421</td>\n",
       "      <td>2.557691</td>\n",
       "      <td>0.684623</td>\n",
       "      <td>1.389368</td>\n",
       "      <td>0.938905</td>\n",
       "      <td>-1.107879</td>\n",
       "      <td>-3.008717</td>\n",
       "      <td>-2.574542</td>\n",
       "      <td>2.074565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.483505</td>\n",
       "      <td>3.740260</td>\n",
       "      <td>3.714517</td>\n",
       "      <td>-1.483981</td>\n",
       "      <td>-0.932160</td>\n",
       "      <td>-1.223809</td>\n",
       "      <td>-0.561965</td>\n",
       "      <td>-2.268734</td>\n",
       "      <td>0.385098</td>\n",
       "      <td>-1.309618</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>1.392407</td>\n",
       "      <td>-1.410966</td>\n",
       "      <td>0.480299</td>\n",
       "      <td>3.181608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.072733</td>\n",
       "      <td>-1.089972</td>\n",
       "      <td>-2.044674</td>\n",
       "      <td>-0.989411</td>\n",
       "      <td>4.432155</td>\n",
       "      <td>3.538149</td>\n",
       "      <td>0.327935</td>\n",
       "      <td>-1.656317</td>\n",
       "      <td>1.972287</td>\n",
       "      <td>2.179859</td>\n",
       "      <td>-3.130990</td>\n",
       "      <td>1.615621</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>1.519352</td>\n",
       "      <td>-1.185252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.102420</td>\n",
       "      <td>-0.533239</td>\n",
       "      <td>0.310449</td>\n",
       "      <td>6.574843</td>\n",
       "      <td>1.080824</td>\n",
       "      <td>-0.322345</td>\n",
       "      <td>0.917680</td>\n",
       "      <td>1.597076</td>\n",
       "      <td>0.692811</td>\n",
       "      <td>-2.357757</td>\n",
       "      <td>0.643224</td>\n",
       "      <td>3.000754</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>-0.434549</td>\n",
       "      <td>1.308862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069206</td>\n",
       "      <td>-1.513340</td>\n",
       "      <td>0.906819</td>\n",
       "      <td>0.405307</td>\n",
       "      <td>-0.203222</td>\n",
       "      <td>2.936823</td>\n",
       "      <td>-0.677617</td>\n",
       "      <td>0.827757</td>\n",
       "      <td>1.834682</td>\n",
       "      <td>-0.723978</td>\n",
       "      <td>0.083857</td>\n",
       "      <td>-3.246837</td>\n",
       "      <td>-1.006093</td>\n",
       "      <td>0.247815</td>\n",
       "      <td>-0.204260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pc1       pc2       pc3       pc4       pc5       pc6       pc7  \\\n",
       "0 -6.316805 -3.993883  2.608049 -1.799177  0.421655 -0.185647 -0.866421   \n",
       "1 -3.483505  3.740260  3.714517 -1.483981 -0.932160 -1.223809 -0.561965   \n",
       "2 -5.072733 -1.089972 -2.044674 -0.989411  4.432155  3.538149  0.327935   \n",
       "3  9.102420 -0.533239  0.310449  6.574843  1.080824 -0.322345  0.917680   \n",
       "4  0.069206 -1.513340  0.906819  0.405307 -0.203222  2.936823 -0.677617   \n",
       "\n",
       "        pc8       pc9      pc10      pc11      pc12      pc13      pc14  \\\n",
       "0  2.557691  0.684623  1.389368  0.938905 -1.107879 -3.008717 -2.574542   \n",
       "1 -2.268734  0.385098 -1.309618  0.762300  1.392407 -1.410966  0.480299   \n",
       "2 -1.656317  1.972287  2.179859 -3.130990  1.615621  0.998560  1.519352   \n",
       "3  1.597076  0.692811 -2.357757  0.643224  3.000754  0.006578 -0.434549   \n",
       "4  0.827757  1.834682 -0.723978  0.083857 -3.246837 -1.006093  0.247815   \n",
       "\n",
       "       pc15  diagadhd  \n",
       "0  2.074565         0  \n",
       "1  3.181608         0  \n",
       "2 -1.185252         0  \n",
       "3  1.308862         0  \n",
       "4 -0.204260         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('df_for_RF.csv')\n",
    "#df1 = pd.read_csv('df_1_outlier.csv')\n",
    "#df = pd.read_csv('finaldf_orderd_withzeros.csv')\n",
    "#df = pd.read_csv('df_eng.csv')\n",
    "df = pd.read_csv('PCA_results.csv')\n",
    "df.drop('Unnamed: 0', axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71\n",
       "1    46\n",
       "Name: diagadhd, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = df['diagadhd'].value_counts(sort = 1)\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:14].values\n",
    "y = df.iloc[:,15].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define dependent and independent variables(adverserial validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# split data into X and y\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,2:], data['diagadhd'], test_size=0.3)\n",
    "training_set, test_set = train_test_split(df, test_size = 0.2, random_state = 3)\n",
    "X_train = training_set.iloc[:,3:].values\n",
    "y_train = training_set.iloc[:,1].values\n",
    "X_test = test_set.iloc[:,3:].values\n",
    "y_test = test_set.iloc[:,1].values\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grid search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# number of trees in RF\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 91)]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_sample_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_sample_leaf = [1, 2, 4]\n",
    "\n",
    "# method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_sample_split,\n",
    "              'min_samples_leaf': min_sample_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "#pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the random grid to search for best hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# random search of parameters using 3 fold corss validation \n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 2, verbose = 2, random_state = 15, n_jobs = -1)\n",
    "\n",
    "# fit the random search model\n",
    "rf_random.fit(X, y)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 72, random_state = 15, min_samples_split=10, min_samples_leaf=4, max_features='sqrt', \n",
    "                  max_depth=30, bootstrap= False)\n",
    "rf.fit(X_train, y_train)\n",
    "#rf.score(X_test, y_test)\n",
    "print('the random forest score without CV: ', rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "the random forest score with CV:  0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose = 2, random_state = 15, n_jobs = -1)\n",
    "rf_random.fit(X, y)\n",
    "\n",
    "print('the random forest score with CV: ', rf_random.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nested grid search with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}# Create a based model\n",
    "rf = RandomForestClassifier()# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 80,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 5,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5485ea7018af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "heatmap = sns.heatmap(confusion, annot=True, cmap='Oranges')\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(),\n",
    "                            rotation=0, ha='right')\n",
    "\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(),\n",
    "                            rotation=0, ha='right')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('RF Model Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive\n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "\n",
    "print('the sensitivity if this model is: ', TP / float(TP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "\n",
    "print('the specificity if this model is: ', TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score1 = model.predict_proba(X_test)[:,1]\n",
    "y_score2 = model.predict_proba(X_test)[:,1]\n",
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)\n",
    "false_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, y_score2)\n",
    "print('roc_auc_score for DecisionTree: ', roc_auc_score(y_test, y_score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeating RF with k best important features (ExtraTreesClassifier method)\n",
    "#### 50-best: importance>0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "#print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_ , index = df.columns[2:])\n",
    "feat_importances.nlargest(50).plot(kind = 'barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame(feat_importances)\n",
    "feat_imp.reset_index(inplace = True)\n",
    "feat_imp.rename(columns={'index': 'region', 0:'importance'}, inplace = True)\n",
    "feat_imp.sort_values(by='importance', ascending=False,inplace=True)\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = feat_imp.iloc[0:50,:]\n",
    "feat_imp=feat_imp.reset_index()\n",
    "feat_imp.drop('index', axis=1, inplace=True)\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp.to_csv('top50feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with25imp = df[[col for col in feat_imp['region']]]\n",
    "df_with25imp.insert(0, 'Subject_ID', df['Subject_ID'])\n",
    "df_with25imp.insert(1, 'diagadhd', df['diagadhd'])\n",
    "df_with25imp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now applying the Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# split data into X and y\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,2:], data['diagadhd'], test_size=0.3)\n",
    "training_set, test_set = train_test_split(df_with25imp, test_size = 0.2, random_state = 3)\n",
    "                                         \n",
    "X_train = training_set.iloc[:,2:].values\n",
    "y_train = training_set.iloc[:,1].values\n",
    "X_test = test_set.iloc[:,2:].values\n",
    "y_test = test_set.iloc[:,1].values\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# number of trees in RF\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 91)]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_sample_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_sample_leaf = [1, 2, 4]\n",
    "\n",
    "# method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_sample_split,\n",
    "              'min_samples_leaf': min_sample_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "#pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the random grid to search for best hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# random search of parameters using 3 fold corss validation \n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose = 2, random_state = 15, n_jobs = -1)\n",
    "\n",
    "# fit the random search model\n",
    "rf_random.fit(X, y)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 68, random_state = 25, min_samples_split=2, min_samples_leaf=2, max_features='auto', max_depth = 100, bootstrap= False)\n",
    "rf.fit(X_train, y_train)\n",
    "print('the random forest score without CV: ', rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose = 2, random_state = 35, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print('the random forest score with CV: ', rf_random.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "heatmap = sns.heatmap(confusion, annot=True, cmap='Oranges')\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(),\n",
    "                            rotation=0, ha='right')\n",
    "\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(),\n",
    "                            rotation=0, ha='right')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('RF Model Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score1 = model.predict_proba(X_test)[:,1]\n",
    "y_score2 = model.predict_proba(X_test)[:,1]\n",
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)\n",
    "false_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, y_score2)\n",
    "print('roc_auc_score for DecisionTree: ', roc_auc_score(y_test, y_score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive\n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
